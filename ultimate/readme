NN implementation file contains only 1 sort of implementation of the Persistence Homology concept with Neural Networks.

4 total implementations have been worked on. I couldn't upload the file due to size issues.
The 4 ways are:
1. Base Model with Topological Loss
2. Fine-tuned VGG-19 model with Topological Loss
3. Enhanced CNN model with Topological Loss
4. Enhanced CNN model without Topological Loss
You can find the code here https://colab.research.google.com/drive/1rlalyeIvIev01O6BzPehoJ6zdYbd3gX9?usp=sharing

The Topological Loss is incorporated in the Neural Network. You can find the a text file which has the code for this purpose. This is just given to show which part was necessary for this work, it is already in above codes for the different models

INCORPORATION: The neural netowrk using to the loss and gradient function where the likelihood and ground truth they will be divided with a patch size of 65x65
1. We use small patches (65 Ã— 65) instead of big patches/whole image.
2. Training using these localized topological loss can be very efficient via random patch sampling. Specifically, we do not partition the image into patches. Instead, we randomly and densely sample patches which can overlap.The patch size controls the tolerable geometric deformation. 
3. During training, even for a same patch, the diagram Dgm(f), the critical pixels, and the gradients change. 
4. At each epoch, we resample patches, reevaluate their persistence diagrams, and the loss gradients. 
5. After computing topological gradients of all sampled patches from a mini-batch, we aggregate them for backpropagation.
6. Our topological loss compliments cross-entropy loss by combating sampling bias.
7. However, for a small amount of difficult locations (blurred regions), it is much harder to learn to predict correctly. The issue is these locations only take a small portion of training pixel samples. Such disproportion cannot be changed even with more annotated training images. Topological loss essentially identifies these difficult locations during training (as critical pixels). It then forces the network to learn patterns near these locations, at the expense of overfitting and consequently slightly compromised per-pixel accuracy.
8. On the other hand, we stress that topological loss cannot succeed alone. Without cross-entropy loss, inferring topology from a completely random likelihood map is meaningless. Cross-entropy loss finds a reasonable likelihood map so that the topological loss can improve its topology.
